{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "from gensim import models, corpora\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets_loader import dataset_loader\n",
    "import pickle\n",
    "\n",
    "DATASET = 'agn'\n",
    "VERSION = 'v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = dataset_loader(None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 120000\n",
      "Test length: 7600\n"
     ]
    }
   ],
   "source": [
    "# Get dataset (can be obtained from https://github.com/mhjabreel/CharCNN/tree/master/data/ag_news_csv or \n",
    "# from https://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
    "\n",
    "train, test, text_field, label_field = data_loader.get_dataset(DATASET)\n",
    "\n",
    "print('Train length:',str(len(train)))\n",
    "print('Test length:',str(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes correspondence in AG News\n",
    "* 1 - World\n",
    "* 2 - Sports\n",
    "* 3 - Business\n",
    "* 4 - Science and technology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(dataset):\n",
    "    return [example.text for example in dataset.examples]\n",
    "\n",
    "tokenized_data_train = get_tokenized_data(train)\n",
    "tokenized_data_test = get_tokenized_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataset):\n",
    "    return np.array( [example.label for example in dataset.examples] )\n",
    "labels_test = get_labels(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = len(label_field.vocab)\n",
    "\n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data_train)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data_train]\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "corpus_test = [dictionary.doc2bow(text) for text in tokenized_data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topics probabilities\n",
    "def get_topics_probabilities(lda_model):\n",
    "    topic_dist_test = lda_model[corpus_test]\n",
    "    results = []\n",
    "    for lda_topic in range(num_topics):\n",
    "         for real_topic in label_field.vocab.stoi.keys():\n",
    "                y_true = (labels_test == real_topic)\n",
    "                support = sum(y_true)\n",
    "                y_score = [dict(probs)[lda_topic] if lda_topic in dict(probs).keys() else 0. \n",
    "                           for probs in np.array(topic_dist_test) ]\n",
    "                pr_auc = average_precision_score(y_true=y_true, y_score=y_score)          \n",
    "                results.append([lda_topic, real_topic, pr_auc, support])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['lda_topic','real_topic','pr_auc','support']).sort_values('pr_auc', ascending=False)\n",
    "    return results_df\n",
    "\n",
    "results_df = get_topics_probabilities(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1         2     3\n",
      "0  1  1  0.874956  1900\n",
      "1  2  0  0.860896  1900\n",
      "2  4  2  0.643909  1900\n",
      "3  3  3  0.473910  1900\n",
      "Mean performance: 0.7134179210399378\n"
     ]
    }
   ],
   "source": [
    "def get_best_model_and_topics(results_df):\n",
    "    classes = dict(label_field.vocab.stoi)\n",
    "    model_perf_data = []\n",
    "    for row in results_df.values:\n",
    "        class_ = row[1]\n",
    "        if class_ in classes.keys():\n",
    "            lda_topic = row[0]\n",
    "            pr_auc = row[2]\n",
    "            support = row[3]\n",
    "            model_perf_data.append([class_, lda_topic, pr_auc, support])\n",
    "            classes.pop(class_)\n",
    "    model_perf_df = pd.DataFrame( model_perf_data )\n",
    "    mean_perf = model_perf_df[2].mean()\n",
    "    return model_perf_df, mean_perf\n",
    "    \n",
    "model_perf_df, mean_perf = get_best_model_and_topics(results_df)\n",
    "print(model_perf_df)\n",
    "print('Mean performance:', mean_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 0.1 0.1 0.6156956371946853\n",
      "-> 0.1 0.25 0.6709602612284461\n",
      "-> 0.1 0.5 0.8551500256441416\n",
      "-> 0.1 0.75 0.8797275744452677\n",
      "0.1 1 0.7996934359227011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [46:01<3:50:09, 2761.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1.25 0.7614413543488324\n",
      "0.25 0.1 0.8540918321709707\n",
      "-> 0.25 0.25 0.888168441946272\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.1, 0.25, 0.5, 0.75, 1, 1.25 ]\n",
    "etas = [0.1, 0.25, 0.5, 0.75, 1, 1.25 ]\n",
    "model_perf_df_results = {}\n",
    "mean_perf_results = {}\n",
    "all_models = {}\n",
    "\n",
    "best_perf = 0\n",
    "\n",
    "for alpha in tqdm(alphas):\n",
    "    for eta in etas:\n",
    "        lda_model = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=5, distributed=False, alpha=alpha, eta=eta)\n",
    "        results_df = get_topics_probabilities(lda_model)\n",
    "        model_perf_df, mean_perf = get_best_model_and_topics(results_df)\n",
    "        model_perf_df_results[str([alpha, eta])] = model_perf_df\n",
    "        mean_perf_results[str([alpha, eta])] = mean_perf\n",
    "        all_models[str([alpha, eta])] = lda_model\n",
    "        \n",
    "        if mean_perf > best_perf: \n",
    "            best_perf = mean_perf\n",
    "            #lda_model.save('best_model.pickle')\n",
    "            print('->',alpha, eta, best_perf)\n",
    "        else:\n",
    "            print(alpha, eta, mean_perf)\n",
    "        \n",
    "        \n",
    "        pickle.dump( model_perf_df_results, open('model_perf_df_results_'+DATASET+'_'+VERSION+'.pickle', 'wb'))\n",
    "        pickle.dump( mean_perf_results, open('mean_perf_results_'+DATASET+'_'+VERSION+'.pickle', 'wb'))\n",
    "        pickle.dump( all_models, open('all_models_'+DATASET+'_'+VERSION+'.pickle', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
